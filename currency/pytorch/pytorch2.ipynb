{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from CurrencyDataset import CurrencyDataset\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from datetime import datetime\n",
    "import math\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../cleaned/usdrub_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['close']\n",
    "df = df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_length = math.floor(len(df) * 0.7)\n",
    "train_df, test_df = df.iloc[:training_length], df.iloc[training_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CurrencyDataset(train_df,  seq_length = 5)\n",
    "test_data = CurrencyDataset(test_df,  seq_length = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_data, batch_size=8)\n",
    "test_dl = DataLoader(test_data, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:\n",
      "torch.Size([8, 5, 1])\n",
      "torch.Size([8, 1])\n"
     ]
    }
   ],
   "source": [
    "print('Training Set:')\n",
    "for sample in train_dl:  \n",
    "    print(sample['data'].size())\n",
    "    print(sample['target'].size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        predictions = self.linear(lstm_out[:,-1,:])\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = LSTM(1, 4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = next(iter(train_dl))\n",
    "input = input['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 5, 1])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2693],\n",
      "        [0.2692],\n",
      "        [0.2692],\n",
      "        [0.2692],\n",
      "        [0.2692],\n",
      "        [0.2692],\n",
      "        [0.2692],\n",
      "        [0.2692]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = rnn(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural = nn.LSTM(1,4,1)\n",
    "h0 = torch.randn(1,4)\n",
    "c0 = torch.randn(1,4)\n",
    "output, (hn, cn) = neural(input, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = train_data[0]['data'] # type: ignore\n",
    "input\n",
    "h_0 = torch.randn(1,8)\n",
    "c_0 = torch.randn(1,8)\n",
    "output = rnn(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    for step, sample in enumerate(train_dl):\n",
    "        input = sample['data']\n",
    "        target = sample['target']\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        output = rnn(input)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if step % 8 == 7:\n",
    "            last_loss = running_loss / 8 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(step + 1, last_loss))\n",
    "            tb_x = epoch_index * len(train_dl) + step + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 8 loss: 2541.1346435546875\n",
      "  batch 16 loss: 2360.689254760742\n",
      "  batch 24 loss: 484.12160873413086\n",
      "  batch 32 loss: 61.535454750061035\n",
      "  batch 40 loss: 102.98833847045898\n",
      "  batch 48 loss: 15.481367617845535\n",
      "  batch 56 loss: 80.04137241840363\n",
      "  batch 64 loss: 53.18872547149658\n",
      "  batch 72 loss: 24.66588521003723\n",
      "  batch 80 loss: 39.60244011878967\n",
      "  batch 88 loss: 25.922372341156006\n",
      "  batch 96 loss: 14.135270714759827\n",
      "  batch 104 loss: 27.764501333236694\n",
      "  batch 112 loss: 30.694674968719482\n",
      "  batch 120 loss: 12.682252489030361\n",
      "  batch 128 loss: 0.9655935075134039\n",
      "  batch 136 loss: 0.2416038690134883\n",
      "  batch 144 loss: 1.271006291732192\n",
      "  batch 152 loss: 1.3900935761630535\n",
      "  batch 160 loss: 11.456250872462988\n",
      "  batch 168 loss: 19.209280252456665\n",
      "  batch 176 loss: 45.3807168006897\n",
      "  batch 184 loss: 19.16089677810669\n",
      "  batch 192 loss: 11.096888214349747\n",
      "  batch 200 loss: 0.8692103512585163\n",
      "  batch 208 loss: 3.1855915263295174\n",
      "  batch 216 loss: 7.308771222829819\n",
      "  batch 224 loss: 0.8911364190280437\n",
      "  batch 232 loss: 3.3112629055976868\n",
      "  batch 240 loss: 13.480094701051712\n",
      "  batch 248 loss: 113.30975389480591\n",
      "  batch 256 loss: 15.660584926605225\n",
      "  batch 264 loss: 45.20275640487671\n",
      "  batch 272 loss: 18.722770385444164\n",
      "LOSS train 18.722770385444164 valid 164.14846801757812\n",
      "EPOCH 2:\n",
      "  batch 8 loss: 455.6561088562012\n",
      "  batch 16 loss: 33.55405634641647\n",
      "  batch 24 loss: 143.9848928451538\n",
      "  batch 32 loss: 56.32453751564026\n",
      "  batch 40 loss: 63.201768442988396\n",
      "  batch 48 loss: 40.688122034072876\n",
      "  batch 56 loss: 175.08416318893433\n",
      "  batch 64 loss: 114.55873113870621\n",
      "  batch 72 loss: 6.471684619784355\n",
      "  batch 80 loss: 22.999292492866516\n",
      "  batch 88 loss: 20.70470893383026\n",
      "  batch 96 loss: 13.94597065448761\n",
      "  batch 104 loss: 30.335872650146484\n",
      "  batch 112 loss: 33.82769441604614\n",
      "  batch 120 loss: 14.233246564865112\n",
      "  batch 128 loss: 0.7817260306328535\n",
      "  batch 136 loss: 0.2923214421607554\n",
      "  batch 144 loss: 1.231273103505373\n",
      "  batch 152 loss: 1.3737735748291016\n",
      "  batch 160 loss: 11.53596980497241\n",
      "  batch 168 loss: 19.34740674495697\n",
      "  batch 176 loss: 45.518001317977905\n",
      "  batch 184 loss: 19.215258479118347\n",
      "  batch 192 loss: 11.112566411495209\n",
      "  batch 200 loss: 0.869773912243545\n",
      "  batch 208 loss: 3.188476376235485\n",
      "  batch 216 loss: 7.314151301980019\n",
      "  batch 224 loss: 0.891825620085001\n",
      "  batch 232 loss: 3.313069924712181\n",
      "  batch 240 loss: 13.48040571808815\n",
      "  batch 248 loss: 113.30713820457458\n",
      "  batch 256 loss: 15.660666108131409\n",
      "  batch 264 loss: 45.20335030555725\n",
      "  batch 272 loss: 18.723114758729935\n",
      "LOSS train 18.723114758729935 valid 164.14822387695312\n",
      "EPOCH 3:\n",
      "  batch 8 loss: 455.6393241882324\n",
      "  batch 16 loss: 33.5426459312439\n",
      "  batch 24 loss: 60.07291615009308\n",
      "  batch 32 loss: 19.768927943892777\n",
      "  batch 40 loss: 86.65575587749481\n",
      "  batch 48 loss: 50.83300590515137\n",
      "  batch 56 loss: 187.63851833343506\n",
      "  batch 64 loss: 126.95267534255981\n",
      "  batch 72 loss: 3.5410555489361286\n",
      "  batch 80 loss: 16.509779453277588\n",
      "  batch 88 loss: 17.92569375038147\n",
      "  batch 96 loss: 14.384222567081451\n",
      "  batch 104 loss: 34.803642988204956\n",
      "  batch 112 loss: 41.46319627761841\n",
      "  batch 120 loss: 20.31822419166565\n",
      "  batch 128 loss: 0.413748050108552\n",
      "  batch 136 loss: 1.149876518175006\n",
      "  batch 144 loss: 0.6193035133183002\n",
      "  batch 152 loss: 1.8040530309081078\n",
      "  batch 160 loss: 11.290817461907864\n",
      "  batch 168 loss: 19.991411805152893\n",
      "  batch 176 loss: 49.557374358177185\n",
      "  batch 184 loss: 24.024703860282898\n",
      "  batch 192 loss: 15.896934390068054\n",
      "  batch 200 loss: 1.080807015299797\n",
      "  batch 208 loss: 1.6628115363419056\n",
      "  batch 216 loss: 6.006760224699974\n",
      "  batch 224 loss: 0.9139059893786907\n",
      "  batch 232 loss: 3.762541025876999\n",
      "  batch 240 loss: 13.388149362057447\n",
      "  batch 248 loss: 110.1925699710846\n",
      "  batch 256 loss: 17.593231558799744\n",
      "  batch 264 loss: 52.270201444625854\n",
      "  batch 272 loss: 24.265623569488525\n",
      "LOSS train 24.265623569488525 valid 169.74131774902344\n",
      "EPOCH 4:\n",
      "  batch 8 loss: 429.9851760864258\n",
      "  batch 16 loss: 35.219957292079926\n",
      "  batch 24 loss: 161.50569200515747\n",
      "  batch 32 loss: 72.74053299427032\n",
      "  batch 40 loss: 46.93404793739319\n",
      "  batch 48 loss: 30.895066380500793\n",
      "  batch 56 loss: 171.59695291519165\n",
      "  batch 64 loss: 123.631174325943\n",
      "  batch 72 loss: 3.1134354397654533\n",
      "  batch 80 loss: 14.71175342798233\n",
      "  batch 88 loss: 16.199024319648743\n",
      "  batch 96 loss: 13.25770252943039\n",
      "  batch 104 loss: 33.69514441490173\n",
      "  batch 112 loss: 40.84588122367859\n",
      "  batch 120 loss: 20.161093682050705\n",
      "  batch 128 loss: 0.41196190752089024\n",
      "  batch 136 loss: 1.1658435799181461\n",
      "  batch 144 loss: 0.609013456851244\n",
      "  batch 152 loss: 1.8233166188001633\n",
      "  batch 160 loss: 11.270481202751398\n",
      "  batch 168 loss: 19.964136958122253\n",
      "  batch 176 loss: 49.540180921554565\n",
      "  batch 184 loss: 24.02121937274933\n",
      "  batch 192 loss: 15.898345828056335\n",
      "  batch 200 loss: 1.0811403654515743\n",
      "  batch 208 loss: 1.6619547363370657\n",
      "  batch 216 loss: 6.005513668060303\n",
      "  batch 224 loss: 0.9137801080942154\n",
      "  batch 232 loss: 3.7621868699789047\n",
      "  batch 240 loss: 13.38803469017148\n",
      "  batch 248 loss: 110.19276928901672\n",
      "  batch 256 loss: 17.59324550628662\n",
      "  batch 264 loss: 52.270063161849976\n",
      "  batch 272 loss: 24.265548676252365\n",
      "LOSS train 24.265548676252365 valid 169.74122619628906\n",
      "EPOCH 5:\n",
      "  batch 8 loss: 429.9854106903076\n",
      "  batch 16 loss: 35.22004985809326\n",
      "  batch 24 loss: 161.50573205947876\n",
      "  batch 32 loss: 72.74051535129547\n",
      "  batch 40 loss: 46.934066593647\n",
      "  batch 48 loss: 30.89508545398712\n",
      "  batch 56 loss: 171.59701108932495\n",
      "  batch 64 loss: 123.63118958473206\n",
      "  batch 72 loss: 3.1134356781840324\n",
      "  batch 80 loss: 14.711752235889435\n",
      "  batch 88 loss: 16.199013829231262\n",
      "  batch 96 loss: 13.257708489894867\n",
      "  batch 104 loss: 33.69515585899353\n",
      "  batch 112 loss: 40.845882177352905\n",
      "  batch 120 loss: 20.16108924150467\n",
      "  batch 128 loss: 0.41196190752089024\n",
      "  batch 136 loss: 1.1658439673483372\n",
      "  batch 144 loss: 0.609017102047801\n",
      "  batch 152 loss: 1.8233163766562939\n",
      "  batch 160 loss: 11.270477268844843\n",
      "  batch 168 loss: 19.96411120891571\n",
      "  batch 176 loss: 49.5401177406311\n",
      "  batch 184 loss: 24.021193623542786\n",
      "  batch 192 loss: 15.89836323261261\n",
      "  batch 200 loss: 1.0811418294906616\n",
      "  batch 208 loss: 1.6619558986276388\n",
      "  batch 216 loss: 6.0054938942193985\n",
      "  batch 224 loss: 0.9137829914689064\n",
      "  batch 232 loss: 3.762181669473648\n",
      "  batch 240 loss: 13.388046655803919\n",
      "  batch 248 loss: 110.19274258613586\n",
      "  batch 256 loss: 17.59323298931122\n",
      "  batch 264 loss: 52.27004837989807\n",
      "  batch 272 loss: 24.26551565527916\n",
      "LOSS train 24.26551565527916 valid 169.74122619628906\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    rnn.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    rnn.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_dl):\n",
    "            vinputs = data['data']\n",
    "            vtarget = data['target']\n",
    "            voutputs = rnn(vinputs)\n",
    "            vloss = loss_fn(voutputs, vtarget)\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(rnn.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
